{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fallen-indonesia",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preliminary-clearing",
   "metadata": {},
   "source": [
    "Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "north-static",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-annual",
   "metadata": {},
   "source": [
    " Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-approval",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter = '\\t', quoting = 3)\n",
    "# quoting = 3 ; is used to ignore double, to avoid processing and sparsing error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-entrepreneur",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding our dataset\n",
    "\n",
    "print(\"Total number of columns:\", len(dataset.columns),\"\\nTotal number of rows:\", len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-universal",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head(n=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-department",
   "metadata": {},
   "source": [
    "\n",
    "Cleaning the texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "younger-parallel",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # used to simplify sentences, in this case reviews\n",
    "import nltk # nlpk helps to download ensemble of stopwords (words that does not help in the prediction of a review, eg a, and, the, I, etc)\n",
    "nltk.download('stopwords') # downloading the stopwords\n",
    "from nltk.corpus import stopwords # importing the stopword for use\n",
    "from nltk.stem.porter import PorterStemmer # importing stem, used for stemming our reviews, it is used for extracting the root word from a given word eg hateful --- hate.\n",
    "\n",
    "corpus = [] # cleaned reviews will be appended to this list\n",
    "for i in range(0, len(dataset)): # iterating through each row\n",
    "    review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i]) # replacing all punctuations with spaces\n",
    "    review = review.lower() # convert all letters to lower case\n",
    "    review = review.split() # splitting each reviews into the constituent words, this returns a list of the words\n",
    "    ps = PorterStemmer()\n",
    "    all_stopwords = stopwords.words('english') # all ensemble of english stopwords\n",
    "    all_stopwords.remove('not') \n",
    "    review = [ps.stem(word) for word in review if not word in set(all_stopwords)] # applying stemming words that are not stopwords\n",
    "    review = ' '.join(review) # joining the words in each review into one string\n",
    "    corpus.append(review) # appending each cleaned review to the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-closure",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-jackson",
   "metadata": {},
   "source": [
    "Creating the Bag of Words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-facing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 1500) # max size of the sparse matrix ie the max number of words to be included in the sparse matrix. The lower the this number, the more less frequent words in the reviews would be removed. Like Holiday, people's name or towns\n",
    "X = cv.fit_transform(corpus).toarray() # \n",
    "y = dataset.iloc[:, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspended-collect",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-direction",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-revolution",
   "metadata": {},
   "source": [
    "Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graduate-ministry",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.10, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automatic-thompson",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The sizes of the train set, x and y are,\", len(X_train), \",\", len(y_train), ', respectively')\n",
    "print(\"The sizes of the test set, x and y are,\", len(X_test), \",\", len(y_test), ', respectively')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banner-tuesday",
   "metadata": {},
   "source": [
    "## Fitting Naive Bayes to the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-upgrade",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banned-president",
   "metadata": {},
   "source": [
    "Predicting the Test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "young-capacity",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-concentration",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-remainder",
   "metadata": {},
   "source": [
    "Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-cleaners",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "print(\"Prediction Accuracy:\",accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requested-dining",
   "metadata": {},
   "source": [
    "## Using  Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-still",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression # import the  Logistic Regression class from its lib\n",
    "classifier = LogisticRegression (random_state=0)\n",
    "classifier.fit(X_train,y_train)\n",
    "\n",
    "# Predicting the Test set Result \n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "print(\"Prediction Accuracy:\",accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-provincial",
   "metadata": {},
   "source": [
    "## Using K-Nearest Neighbor KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "given-appointment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting classifier to the Training set\n",
    "from sklearn.neighbors import KNeighborsClassifier # importing the KNeighborsClassifier from its lib\n",
    "classifier = KNeighborsClassifier (n_neighbors=5, metric= 'minkowski', p=2 ) # creating an object out of KNeighborsClassifier, press cmd+i for the meaning of the arguement selected. Note they are all important\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set Result \n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "print(\"Prediction Accuracy:\",accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-there",
   "metadata": {},
   "source": [
    "## Using SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-arena",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Support Vector Machine SVM to the Training set\n",
    "from sklearn.svm import SVC \n",
    "classifier = SVC (kernel = 'linear',  random_state = 0, ) # creating an object out of SVC, press cmd+i for the meaning of the arguement selected. Note they are all important. Using kernel = 'linear' will change our classifier to linear\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Predicting the Test set Result \n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "print(\"Prediction Accuracy:\",accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-spiritual",
   "metadata": {},
   "source": [
    "## Using Kernel SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerous-nitrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Kernel SVM to the Training set\n",
    "from sklearn.svm import SVC # importing the kernel svm from its lib\n",
    "classifier = SVC (kernel = 'rbf',   random_state = 0) # creating an object out of SVC, press cmd+i for the meaning of the arguement selected. Note they are all important. Using kernel = 'rbf' will change our classifier to gaussian\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "# Predicting the Test set Result \n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "print(\"Prediction Accuracy:\",accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-arena",
   "metadata": {},
   "source": [
    "## Using Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-party",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Decision Tree Classification to the Training set\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0) # the most used criterion is entropy\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Predicting the Test set Result \n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "print(\"Prediction Accuracy:\",accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-jonathan",
   "metadata": {},
   "source": [
    "## Using Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alike-mitchell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Random Forest Classification to the Training set\n",
    "from sklearn.ensemble import RandomForestClassifier # importing the random forest classifier \n",
    "classifier = RandomForestClassifier (n_estimators=10, criterion = 'entropy', random_state = 0) # the most used criterion is entropy\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Predicting the Test set Result \n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "print(\"Prediction Accuracy:\",accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "photographic-deposit",
   "metadata": {},
   "source": [
    "## Final Words\n",
    "\n",
    "With an accuracy of 0.79 (79%) for the test set, the prediction model built using SVM classifier (kernel=linear) was the best. The second best is another SVM classifier (kernel = 'rbf'). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-finding",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stunning-lithuania",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
